<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> George Jiayuan Gao </title> <meta name="author" content="George Jiayuan Gao"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8F%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ggao22.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">home <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">George</span> Jiayuan Gao </h1> <p class="desc">gegao@seas.upenn.edu</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.png?4550ea1f55beddb73c3cb9ce0145256d" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am a second year Robotics Master’s student in the <a href="https://www.grasp.upenn.edu/" rel="external nofollow noopener" target="_blank">General Robotics, Automation, Sensing &amp; Perception (GRASP) Laboratory</a> at the University of Pennsylvania, advised by <a href="https://nbfigueroa.github.io/" rel="external nofollow noopener" target="_blank">Prof. Nadia Figueroa</a> and <a href="https://www.seas.upenn.edu/~dineshj/" rel="external nofollow noopener" target="_blank">Prof. Dinesh Jayaraman</a>.</p> <p>Previously, I completed my undergraduate studies in Mathematics and Computer Science at Washington University in St. Louis, where I worked with <a href="https://vorobeychik.com/" rel="external nofollow noopener" target="_blank">Prof. Yevgeniy Vorobeychik</a>.</p> <p><b>My research broadly focuses on the <b>generalization of learning-based robot policies</b></b> 🦾.</p> <p>Link to my <a href="/assets/pdf/cv.pdf">CV</a> (last update: June 2025).</p> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%67%65%67%61%6F@%73%65%61%73.%75%70%65%6E%6E.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=dCUjGQ0AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/ggao22" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/ggao1" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> </div> <div class="contact-note"></div> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 15, 2025</th> <td> Our paper <a href="https://arxiv.org/abs/2411.03294" rel="external nofollow noopener" target="_blank">“Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy For Visuomotor Imitation Learning”</a> was <b><b>accepted at IROS 2025</b></b>! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 08, 2025</th> <td> <a href="https://vlmgineer.github.io/release" rel="external nofollow noopener" target="_blank">VLMgineer</a> has been accepted for <b>Oral Spotlight</b> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> at <a href="https://rss-hardware-intelligence.github.io/" rel="external nofollow noopener" target="_blank">RSS 2025 Workshop on Robot Hardware-Aware Intelligence</a>! Come checkout our presentation and poster session if you’re interested! </td> </tr> <tr> <th scope="row" style="width: 20%">May 14, 2025</th> <td> Honored to have received Penn Engineering’s graduate <b><a href="https://www.grasp.upenn.edu/news/2025-grasp-robotics-masters-award-round-up/" rel="external nofollow noopener" target="_blank">Outstanding Research Award</a></b> this year! </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 09, 2024</th> <td> Check out our <b>Spotlight Presentation</b> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> for our <a href="https://sites.google.com/view/ocr-penn" rel="external nofollow noopener" target="_blank">OCR</a> Project at CoRL 2024 Workshop on Lifelong Learning for Home Robots. <a href="https://www.youtube.com/watch?v=-24YYIIPWjM&amp;feature=youtu.be" rel="external nofollow noopener" target="_blank">Video</a> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vlmgineer.avifs" sizes="275px"> <img src="/assets/img/publication_preview/vlmgineer.avifs" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vlmgineer.avifs" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="gao2025vlmgineer" class="col-sm-9"> <div class="title">VLMgineer: Vision Language Models as Robotic Toolsmiths</div> <div class="author"> <em>George Jiayuan Gao<sup>*</sup></em>, <a href="http://imtianyuli.com/" rel="external nofollow noopener" target="_blank">Tianyu Li<sup>*</sup></a>, <a href="https://junyaoshi.github.io/" rel="external nofollow noopener" target="_blank">Junyao Shi</a> , <a href="https://yihanli126.github.io/" rel="external nofollow noopener" target="_blank">Yihan Li<sup>†</sup></a>, <a href="https://zizhe.io/" rel="external nofollow noopener" target="_blank">Zizhe Zhang<sup>†</sup></a>, <a href="https://nbfigueroa.github.io/" rel="external nofollow noopener" target="_blank">Nadia Figueroa</a>, and <a href="https://www.seas.upenn.edu/~dineshj/" rel="external nofollow noopener" target="_blank">Dinesh Jayaraman</a> </div> <div class="periodical"> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <b>Spotlight</b> at RSS Workshop on Robot Hardware-Aware Intelligence, <em>2025</em>. </div> <div class="periodical"> </div> <div class="links"> <a href="https://vlmgineer.github.io/release" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Website</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://vlmgineer.github.io/static/VLMgineer.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Tool design and use reflect the ability to understand and manipulate the physical world through creativity, planning, and foresight. As such, it is often regarded as a measurable indicator of cognitive intelligence across biological species. While much of today’s research on robotics intelligence focuses on generating better control strategies, inventing smarter tools offers a complementary form of physical intelligence: moving the problem-solving onus into the tool’s geometry so that control becomes simpler.This motivates us to ask: can today’s foundation models offer useful priors to automatically invent—and effectively wield—such tools? We present VLMgineer, a framework that harnesses the creativity of Vision–Language Models (VLMs) together with evolutionary search to co-design physical tools and the control policies that operate them. We evaluate VLMgineer on a diverse benchmark of everyday manipulation scenarios that demand creative tool design and use. Across this suite, VLMgineer consistently discovers tools and policies that solve tasks more effectively and innovatively, transforming challenging robotics problems into straightforward executions. It also consistently outperforms VLM-generated designs from human specifications and existing human-crafted tools for everyday tasks. To facilitate future research on automated tool invention, we will release our benchmark and code.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ocr_preview-480.webp 480w,/assets/img/publication_preview/ocr_preview-800.webp 800w,/assets/img/publication_preview/ocr_preview-1400.webp 1400w," type="image/webp" sizes="275px"> <img src="/assets/img/publication_preview/ocr_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ocr_preview.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="gao2025ocr" class="col-sm-9"> <div class="title">Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy For Visuomotor Imitation Learning</div> <div class="author"> <em>George Jiayuan Gao</em>, <a href="http://imtianyuli.com/" rel="external nofollow noopener" target="_blank">Tianyu Li</a>, and <a href="https://nbfigueroa.github.io/" rel="external nofollow noopener" target="_blank">Nadia Figueroa</a> </div> <div class="periodical"> International Conference on Intelligent Robots and Systems (IROS), <em>2025</em>. </div> <div class="periodical"> </div> <div class="links"> <a href="https://sites.google.com/view/ocr-penn" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Website</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2411.03294" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2411.03294" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=-24YYIIPWjM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We propose an object-centric recovery policy framework to address the challenges of out-of-distribution (OOD) scenarios in visuomotor policy learning. Previous behavior cloning (BC) methods rely heavily on a large amount of labeled data coverage, failing in unfamiliar spatial states. Without relying on extra data collection, our approach learns a recovery policy constructed by an inverse policy inferred from object keypoint manifold gradient in the original training data. The recovery policy serves as a simple add-on to any base visuomotor BC policy, agnostic to a specific method, guiding the system back towards the training distribution to ensure task success even in OOD situations. We demonstrate the effectiveness of our object-centric framework in both simulation and real robot experiments, achieving an improvement of 77.7% over the base policy in OOD. </p> </div> </div> </div> </li> </ol> </div> <h2> <a href="/publications/" style="color: inherit">Selected Projects</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/eureka_manip-480.webp 480w,/assets/img/publication_preview/eureka_manip-800.webp 800w,/assets/img/publication_preview/eureka_manip-1400.webp 1400w," type="image/webp" sizes="275px"> <img src="/assets/img/publication_preview/eureka_manip.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="eureka_manip.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="eureka_manip" class="col-sm-9"> <div class="title">(On-Going) Eureka for Manipulation: Real-World Dexterous Agent via Large-Scale Reinforcement Learning</div> <div class="author"> </div> <div class="periodical"> Training a skilled manipulation agent with RL in simulation that can zero-shot transfer to the real world is hard. The question is: does this get any easier when we add LLM in the loop and utilize ginormous levels of computing power, such as hundreds of Nvidia’s latest generation of data-center GPUs?, <em>2025</em>. </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/converger-480.webp 480w,/assets/img/publication_preview/converger-800.webp 800w,/assets/img/publication_preview/converger-1400.webp 1400w," type="image/webp" sizes="275px"> <img src="/assets/img/publication_preview/converger.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="converger.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="novel_view_action_synthesis" class="col-sm-9"> <div class="title">(On-Going) Stable Visuomotor Policy from a Single Demo: Elastic Action Synthesis Data Augmentation</div> <div class="author"> </div> <div class="periodical"> We propose a methodology that uses our in-house Elastic-Motion-Policy, enabling the training of visuomotor policies with full spatial generalization from only a single demonstration, <em>2024</em>. </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gdn-act-480.webp 480w,/assets/img/publication_preview/gdn-act-800.webp 800w,/assets/img/publication_preview/gdn-act-1400.webp 1400w," type="image/webp" sizes="275px"> <img src="/assets/img/publication_preview/gdn-act.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gdn-act.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="gdn-act" class="col-sm-9"> <div class="title">Novel Environment Transfer of Visuomotor Policy Via Object-Centric Domain-Randomization</div> <div class="author"> </div> <div class="periodical"> Proposed <b>GDN-ACT</b>, a novel, scalable approach that enables <b>zero-shot generalization</b> of visuomotor policies across unseen environments, using a pre-trained state-space mapping for object localization, <em>May</em> <em>2025</em>. </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/gdn-act.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gmot-480.webp 480w,/assets/img/publication_preview/gmot-800.webp 800w,/assets/img/publication_preview/gmot-1400.webp 1400w," type="image/webp" sizes="275px"> <img src="/assets/img/publication_preview/gmot.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gmot.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="gmot" class="col-sm-9"> <div class="title">Modular Gait Optimization: From Unit Moves to Multi-Step Trajectory in Bipedal Systems</div> <div class="author"> </div> <div class="periodical"> Proposed the Gait Modularization and Optimization Technique (GMOT), which leverages modular <b>unit gaits as initialization</b> for Hybrid Direct Collocation (HDC), reducing sensitivity to constraints and enhancing computational stability across various gaits, including walking, running, and hopping, <em>Dec</em> <em>2023</em>. </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/gmot.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/QingquanBao/2DBiped" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yvxaiver-480.webp 480w,/assets/img/publication_preview/yvxaiver-800.webp 800w,/assets/img/publication_preview/yvxaiver-1400.webp 1400w," type="image/webp" sizes="275px"> <img src="/assets/img/publication_preview/yvxaiver.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yvxaiver.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="yvxaiver" class="col-sm-9"> <div class="title">Miniature City Autonomous Driving Platform Development with Real-Time Vision-Based Lane-Following</div> <div class="author"> </div> <div class="periodical"> Developed the drive stack for Washington University’s inaugural miniature city autonomous driving platform by developing the vision-based lane-following pipeline, <em>May</em> <em>2023</em>. </div> <div class="periodical"> </div> <div class="links"> <a href="https://github.com/ggao22/yvxavier" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 George Jiayuan Gao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>